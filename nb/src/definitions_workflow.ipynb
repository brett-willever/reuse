{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sqlparse\n",
    "from functools import lru_cache\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import duckdb\n",
    "from fastapi import FastAPI\n",
    "from duckdb import DuckDBPyConnection as DuckCall\n",
    "from typing import List, Union, Dict, Any, AnyStr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel as ModelResponseObject\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class UserLLMRequest(BaseModel):\n",
    "    input_schema: AnyStr\n",
    "\n",
    "\n",
    "class ILLMReponse(BaseModel):\n",
    "    original_prompt: Dict[str, str]\n",
    "    response: List[Union[Dict[str, Any], None]]\n",
    "\n",
    "\n",
    "class LLMInterface(object): ...\n",
    "\n",
    "\n",
    "class OpenAILLMInterface(ChatOpenAI):\n",
    "    inline_prompt: str\n",
    "    role_prompt: str\n",
    "\n",
    "\n",
    "class OneHotEncode(BaseModel): ...\n",
    "\n",
    "\n",
    "class ModelReponse(ModelResponseObject):\n",
    "    original_prompt: OpenAILLMInterface\n",
    "    response: List[Union[Dict[str, Any], None]]\n",
    "\n",
    "\n",
    "def encoder(_: OneHotEncode) -> ...: ...\n",
    "\n",
    "\n",
    "v1 = FastAPI()\n",
    "_init_cache: Dict = dict()\n",
    "duck_call = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "\n",
    "\n",
    "@v1.get(\"/v1/query\")\n",
    "def invoke_duck_call(query: str) -> List[Any]:\n",
    "    return preload_exec(duck_call, query)\n",
    "\n",
    "\n",
    "@v1.post(\"/v1/experimental/llm\", response_model=ILLMReponse)\n",
    "async def llm_interaction(request: UserLLMRequest) -> ILLMReponse:\n",
    "    model_interface = OpenAILLMInterface()\n",
    "    user_request_appendment = request\n",
    "    contractual_model = model_interface.with_structured_output(ModelReponse)\n",
    "    llm_response = contractual_model.invoke(user_request_appendment)\n",
    "    return ILLMReponse(\n",
    "        original_prompt=llm_response.original_prompt, response=llm_response.response\n",
    "    )\n",
    "\n",
    "\n",
    "# @on_start\n",
    "# def _on_start() -> DuckCall:\n",
    "#     return duckdb.connect(database=\":memory:\", read_only=False)\n",
    "\n",
    "class OneHotEncodingHandler(object):\n",
    "    @staticmethod\n",
    "    @v1.post(\"/v1/experimental/onehot\", response_model=...)\n",
    "    def onehot_encode(data: Any) -> ...:\n",
    "        # Implement one-hot encoding logic here\n",
    "        pass\n",
    "\n",
    "\n",
    "class SubmitTokensHandler(object):\n",
    "    @staticmethod\n",
    "    @v1.post(\"/v1/experimental/submit_tokens\", response_model=ILLMReponse)\n",
    "    async def submit_tokens(request: UserLLMRequest) -> ILLMReponse:\n",
    "        model_interface = OpenAILLMInterface()\n",
    "        user_request_appendment = request\n",
    "        contractual_model = model_interface.with_structured_output(ModelReponse)\n",
    "        llm_response = contractual_model.invoke(user_request_appendment)\n",
    "        return ILLMReponse(\n",
    "            original_prompt=llm_response.original_prompt, response=llm_response.response\n",
    "        )\n",
    "    \n",
    "\n",
    "# @lru_cache(maxsize=480)\n",
    "def _cache(tables: List[str]) -> List[str]:\n",
    "    rt: List = list()\n",
    "    tables = [rt.append(t) for t in tables if not _init_cache.get(t, None)]\n",
    "    for t in tables:\n",
    "        if not _init_cache.get(t, None):\n",
    "            rt.append(t)\n",
    "        _init_cache[t] = datetime.now()\n",
    "    return rt\n",
    "\n",
    "\n",
    "def find_tables(query: str) -> List[str]:\n",
    "    tables = list()\n",
    "    parsed = sqlparse.parse(query)[0]\n",
    "    for t in parsed.tokens:\n",
    "        if isinstance(t, sqlparse.sql.Identifier):\n",
    "            tables.append(t.value)\n",
    "    return tables\n",
    "\n",
    "\n",
    "def preload_tables(connection: DuckCall, tables: List[str]) -> List[bool]:\n",
    "    cached = list()\n",
    "    for t in tables:\n",
    "        q = f\"create table {t} as select * from read_parquet('{t});\"\n",
    "        try:\n",
    "            connection.execute(q).fetchall()\n",
    "            cached.append(False)\n",
    "        except:\n",
    "            cached.append(True)\n",
    "    return cached\n",
    "\n",
    "\n",
    "def preload_exec(connection: DuckCall, query: str) -> List[Tuple]:\n",
    "    tables = find_tables(query)\n",
    "    tables = _cache(tables)\n",
    "    _ = preload_tables(connection, tables)\n",
    "    return connection.execute(query).fetchall()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(v1, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
